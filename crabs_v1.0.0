#! /usr/bin/env python3

################################################
################ IMPORT MODULES ################
################################################
import argparse
import subprocess as sp
import os
import shutil
import collections
from collections import Counter
from Bio import SeqIO
from Bio.Seq import Seq
from function.module_1 import esearch_fasta, efetch_seqs_from_webenv, ncbi_formatting, mitofish_download, mitofish_format, embl_download, embl_fasta_format, embl_crabs_format, bold_download, bold_format, check_accession, append_primer_seqs, generate_header, merge_databases
from function.module_3 import tax2dict, get_accession, acc_to_dict, get_lineage, final_lineage_comb
from function.module_5 import split_db_by_taxgroup, num_spec_seq_taxgroup, horizontal_barchart, get_amp_length, amplength_figure

################################################
########### MODULE DATABASE DOWNLOAD ###########
################################################

## function download data from online databases
def db_download(args):
    SOURCE = args.source
    DATABASE = args.database
    QUERY = args.query
    OUTPUT = args.output
    ORIG = args.orig
    EMAIL = args.email
    BATCHSIZE = args.batchsize

    ## download taxonomy data from NCBI
    if SOURCE == 'taxonomy':
        print('\ndownloading taxonomy information')
        url_acc2taxid = 'ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/nucl_gb.accession2taxid.gz'
        url_taxdump = 'ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz'        
        results = sp.run(['wget', url_acc2taxid])
        results = sp.run(['gunzip', 'nucl_gb.accession2taxid.gz'])
        results = sp.run(['wget', url_taxdump])
        results = sp.run(['tar', '-zxvf', 'taxdump.tar.gz'])
        print('removing intermediary files\n')
        files_to_remove = ['citations.dmp', 'delnodes.dmp', 'division.dmp', 'gencode.dmp', 'merged.dmp', 'gc.prt', 'readme.txt', 'taxdump.tar.gz']
        for file in files_to_remove:
            os.remove(file)

    ## download sequencing data from NCBI
    elif SOURCE == 'ncbi':
        if all(v is not None for v in [DATABASE, QUERY, OUTPUT, EMAIL]):
            print('\ndownloading sequences from NCBI')
            print('looking up the number of sequences that match the query')
            search_record = esearch_fasta(QUERY, DATABASE, EMAIL)
            print('found {} matching sequences'.format(search_record['Count']))
            print('starting the download')
            batch_size = BATCHSIZE
            filename = 'CRABS_ncbi_download.fasta'
            fetch_seqs = efetch_seqs_from_webenv(search_record, DATABASE, EMAIL, batch_size, filename)
            print(f'downloaded {fetch_seqs} sequences from NCBI')
            print('formatting the downloaded sequencing file to CRABS format')
            format_seqs = ncbi_formatting(OUTPUT, ORIG)
            print(f'written {format_seqs} sequences to {OUTPUT}\n')
        else:
            print('\nnot all parameters have an input value\n')

    ## download sequencing data from EMBL
    elif SOURCE == 'embl':
        if all(v is not None for v in [DATABASE, OUTPUT]):
            print('\ndownloading sequences from EMBL')
            dl_files = embl_download(DATABASE)
            fasta_file = embl_fasta_format(dl_files)
            print(f'formatting intermediary file to CRABS format')
            crabs_file = embl_crabs_format(fasta_file, OUTPUT, ORIG)
            print(f'written {crabs_file} sequences to {OUTPUT}\n')
        else:
            print('\nnot all parameters have an input value\n')

    ## download sequencing data from MitoFish
    elif SOURCE == 'mitofish':
        if all(v is not None for v in [OUTPUT]):
            print('\ndownloading sequences from the MitoFish database')
            url = 'http://mitofish.aori.u-tokyo.ac.jp/files/complete_partial_mitogenomes.zip'
            dl_file = mitofish_download(url)
            print(f'formatting {dl_file} to CRABS format')
            mitoformat = mitofish_format(dl_file, OUTPUT, ORIG)
            print(f'written {mitoformat} sequences to {OUTPUT}\n')
        else:
            print('\nnot all parameters have an input value\n')

    ## download sequencing data from BOLD
    elif SOURCE == 'bold':
        if all(v is not None for v in [DATABASE, OUTPUT]):
            print('\ndownloading sequences from BOLD')
            bold_file = bold_download(DATABASE)
            print(f'downloaded {bold_file} sequences from BOLD')
            print(f'formatting {bold_file} sequences to CRABS format')
            boldformat = bold_format(OUTPUT, ORIG)
            print(f'written {boldformat} sequences to {OUTPUT}\n')
        else:
            print('\nnot all parameters have an input value\n')

## function: import existing or custom database
def db_import(args):
    INPUT = args.input
    HEADER = args.header
    OUTPUT = args.output
    FWD = args.fwd
    REV = args.rev
    DELIM = args.delim

    ## process file with accession number in header
    if HEADER == 'accession':
        if all(v is not None for v in [INPUT, OUTPUT, DELIM]):
            print(f'\nchecking correct formatting of accession numbers in {INPUT}')
            incorrect_accession = check_accession(INPUT, OUTPUT, DELIM)
            if len(incorrect_accession) != 0:
                print('found incorrectly formatted accession numbers, please check file: "incorrect_accession_numbers.txt"')
                with open('incorrect_accession_numbers.txt', 'w') as fout:
                    for item in incorrect_accession:
                        fout.write(item + '\n')
            if all(v is not None for v in [FWD, REV]):
                print(f'appending primer sequences to {OUTPUT}')
                numseq = append_primer_seqs(OUTPUT, FWD, REV)
                print(f'added primers to {numseq} sequences in {OUTPUT}\n')
            else:
                print('')
        else:
            print('\nnot all parameters have an input value\n')

    ## process file with species info in header
    elif HEADER == 'species':
        if all(v is not None for v in [INPUT, OUTPUT, DELIM]):
            print(f'\ngenerating new sequence headers for {INPUT}')
            num_header = generate_header(INPUT, OUTPUT, DELIM)
            print(f'generated {num_header} headers for {OUTPUT}')
            if all(v is not None for v in [FWD, REV]):
                print(f'appending primer sequences to {OUTPUT}')
                numseq = append_primer_seqs(OUTPUT, FWD, REV)
                print(f'added primers to {numseq} sequences in {OUTPUT}\n')
            else:
                print('')
        else:
            print('\nnot all parameters have an input value\n')
    else:
        print('\nplease specify header information: "accession" and "species"\n')

## function: merge multiple databases
def db_merge(args):
    INPUT = args.input
    UNIQ = args.uniq
    OUTPUT = args.output

    if UNIQ != '':
        print('\nmerging all fasta files and discarding duplicate sequence headers')
        num_uniq = merge_databases(INPUT, OUTPUT)
        print(f'written {num_uniq} sequences to {OUTPUT}\n')
    else:
        print('\nmerging all fasta files and keeping duplicate sequence headers')
        with open(OUTPUT, 'w') as fout:
            for file in INPUT:
                num = len(list(SeqIO.parse(file, 'fasta')))
                print(f'found {num} sequences in {file}')
                with open(file, 'r') as fin:
                    for line in fin:
                        fout.write(line)
        num = len(list(SeqIO.parse(OUTPUT, 'fasta')))
        print(f'written {num} sequences to {OUTPUT}\n')


################################################
############# MODULE IN SILICO PCR #############
################################################

## function: in silico PCR
def insilico_pcr(args):
    FWD = args.fwd
    REV = args.rev
    INPUT = args.input
    ERROR = args.error
    OUTPUT = args.output

    ## reverse complement reverse primer sequence
    REV_CORRECT = str(Seq(REV).reverse_complement())

    ## setting variable names using the info from user input
    TRIMMED_INIT = 'init_trimmed.fasta'
    UNTRIMMED_INIT = 'init_untrimmed.fasta'
    REVCOMP_UNTRIMMED_INIT = 'revcomp_untrimmed.fasta'
    TRIMMED_REVCOMP = 'revcomp_trimmed.fasta'
    UNTRIMMED_REVCOMP = 'untrimmed_revcomp.fasta'

    OVERLAP = str(min([len(FWD), len(REV_CORRECT)]))
    ADAPTER = FWD + '...' + REV_CORRECT

    ## run cutadapt on downloaded fasta file
    count_init = len(list(SeqIO.parse(INPUT, 'fasta')))
    print('\nrunning in silico PCR on fasta file containing {} sequences'.format(count_init))
    cmnd_cutadapt_1 = ['cutadapt', '-g', ADAPTER, '-o', TRIMMED_INIT, INPUT, '--untrimmed-output', UNTRIMMED_INIT, '--no-indels', '-e', ERROR, '--overlap', OVERLAP, '--quiet']
    sp.call(cmnd_cutadapt_1)
    count_trimmed_init = len(list(SeqIO.parse(TRIMMED_INIT, 'fasta')))
    print('found primers in {} sequences'.format(count_trimmed_init))

    ## run vsearch to reverse complement untrimmed sequences
    if count_trimmed_init < count_init:
        count_untrimmed_init = len(list(SeqIO.parse(UNTRIMMED_INIT, 'fasta')))
        print('reverse complementing {} untrimmed sequences'.format(count_untrimmed_init))
        cmnd_vsearch_revcomp = ['vsearch', '--fastx_revcomp', UNTRIMMED_INIT, '--fastaout', REVCOMP_UNTRIMMED_INIT, '--quiet']
        sp.call(cmnd_vsearch_revcomp)

        ## run cutadapt on reverse complemented untrimmed sequences
        print('running in silico PCR on {} reverse complemented untrimmed sequences'.format(count_untrimmed_init))
        cmnd_cutadapt_2 = ['cutadapt', '-g', ADAPTER, '-o', TRIMMED_REVCOMP, REVCOMP_UNTRIMMED_INIT, '--untrimmed-output', UNTRIMMED_REVCOMP, '--no-indels', '-e', ERROR, '--overlap', OVERLAP, '--quiet']
        sp.call(cmnd_cutadapt_2)
        count_trimmed_second = len(list(SeqIO.parse(TRIMMED_REVCOMP, 'fasta')))
        print('found primers in {} sequences\n'.format(count_trimmed_second))

        ## concatenate both trimmed files
        with open(OUTPUT, 'wb') as wfd:
            for f in [TRIMMED_INIT, TRIMMED_REVCOMP]:
                with open(f, 'rb') as fd:
                    shutil.copyfileobj(fd, wfd)
        
        ## remove intermediary files
        files = [TRIMMED_INIT, UNTRIMMED_INIT, REVCOMP_UNTRIMMED_INIT, TRIMMED_REVCOMP, UNTRIMMED_REVCOMP]
        for file in files:
            os.remove(file)
    
    ## don't run reverse complement when initial in silico PCR trims all sequences
    else:
        print('all sequences trimmed, no reverse complement step\n')
        results = sp.run(['mv', TRIMMED_INIT, OUTPUT])
        os.remove(UNTRIMMED_INIT)

################################################
########## MODULE TAXONOMY ASSIGNMENT ##########
################################################
## function: get taxonomic lineage for each accession number
def assign_tax(args):
    INPUT = args.input
    OUTPUT = args.output
    ACC2TAX = args.acc2tax
    TAXID = args.taxid
    NAME = args.name

    ## process initial files
    print(f'\nretrieving accession numbers from {INPUT}')
    accession = get_accession(INPUT)
    print(f'found {len(accession)} accession numbers in {INPUT}')
    acc2tax, taxid, name, no_acc = tax2dict(ACC2TAX, TAXID, NAME, accession)
    print(f'processed {len(acc2tax)} entries in {ACC2TAX}')
    print(f'processed {len(taxid)} entries in {TAXID}')
    print(f'processed {len(name)} entries in {NAME}')


    ## get taxonomic lineage
    print(f'assigning a tax ID number to {len(accession)} accession numbers from {INPUT}')
    acc_taxid_dict, taxid_list = acc_to_dict(accession, acc2tax, no_acc)
    print(f'{len(acc_taxid_dict)} accession numbers resulted in {len(taxid_list)} unique tax ID numbers')
    print(f'generating taxonomic lineages for {len(taxid_list)} tax ID numbers')
    lineage = get_lineage(taxid_list, taxid, name)
    print(f'assigning a taxonomic lineage to {len(accession)} accession numbers')
    final_lineage = final_lineage_comb(acc_taxid_dict, lineage, INPUT, OUTPUT)
    print(f'written {len(final_lineage)} entries to {OUTPUT}\n')

################################################
########### MODULE DATABASE CLEAN-UP ###########
################################################

## function: dereplicating the database
def dereplicate(args):
    INPUT = args.input
    OUTPUT = args.output
    METHOD = args.method

    ## dereplicate strict (only unique sequences)
    if METHOD == 'strict':
        print(f'\nstrict dereplication of {INPUT}, only keeping unique sequences')
        uniq_seqs = {}
        uniq_line = []
        count = 0
        added = 0
        with open(INPUT, 'r') as file_in:
            for line in file_in:
                count = count + 1
                lines = line.rstrip('\n')
                seq = lines.split('\t')[9]
                if seq not in uniq_seqs:
                    added = added + 1
                    uniq_seqs[seq] = seq
                    uniq_line.append(line)
        print(f'found {count} sequences in {INPUT}')
        print(f'written {added} sequences to {OUTPUT}\n')
        with open(OUTPUT, 'w') as file_out:
            for line in uniq_line:
                file_out.write(line)

    ## dereplicate single species (one sequence per species)
    elif METHOD == 'single_species':
        print(f'\ndereplicating {INPUT}, only keeping a single sequence per species')
        uniq_spec = {}
        uniq_line = []
        count = 0
        added = 0
        with open(INPUT, 'r') as file_in:
            for line in file_in:
                count = count + 1
                lines = line.rstrip('\n')
                species = lines.split('\t')[8].split(',')[2]
                if species not in uniq_spec:
                    added = added + 1
                    uniq_spec[species] = species 
                    uniq_line.append(line)
        print(f'found {count} sequences in {INPUT}')
        print(f'written {added} sequences to {OUTPUT}\n')
        with open(OUTPUT, 'w') as file_out:
            for line in uniq_line:
                file_out.write(line)

    ## dereplicate unique species (all unique sequences per species)
    elif METHOD == 'uniq_species':
        print(f'\ndereplicating {INPUT}, keeping all unique sequences per species')
        mydict = collections.defaultdict(list)
        count = 0
        added = 0
        with open(INPUT, 'r') as file_in:
            for line in file_in:
                count = count + 1
                lines = line.rstrip('\n')
                spec = lines.split('\t')[8].split(',')[2]
                seq = lines.split('\t')[9]
                line_id = lines
                seq_dicts = []
                for item in mydict[spec]:
                    seq_dict = item.rsplit('\t', 1)[1]
                    seq_dicts.append(seq_dict)
                if seq not in seq_dicts:
                    added = added + 1
                    mydict[spec].append(line_id)
        print(f'found {count} sequences in {INPUT}')
        print(f'written {added} sequences to {OUTPUT}\n')
        with open(OUTPUT, 'w') as file_out:
            for k, v in mydict.items():
                for i in v:
                    file_out.write(i + '\n')

    ## dereplicate concensus species (generate concensus sequence for each species)
    elif METHOD == 'consensus':
        print('still to add...')

    ## unknown method specified
    else:
        print('\nplease specify one of the accepted dereplication methods: "strict", "single_species", "uniq_species"\n')

## function: sequence cleanup
def db_filter(args):
    MINLEN = args.minlen
    MAXLEN = args.maxlen
    MAXNS = args.maxns
    INPUT = args.input
    OUTPUT = args.output
    DISCARD = args.discard
    ENV = args.env
    SPEC = args.spec
    NANS = args.nans

    ## set filtering parameters
    print(f'\nfiltering parameters:\nremoving sequences shorter than {MINLEN} and longer than {MAXLEN}\nremoving sequences containing more than {MAXNS} "N"')
    if ENV == 'no':
        env = 100
        print('keeping environmental sequences')
    else:
        env = 0
        print('removing environmental sequences')
    if SPEC == 'no':
        print('keeping sequences unclassified at species level')
        spec = 100
    else:
        spec = 0
        print('removing sequences without a species ID')
    if NANS == 'no':
        nans = 100
        print('keeping sequences with missing taxonomic information')
    else:
        nans = int(NANS)
        print(f'removing sequences with missing information for {NANS} taxonomic levels')
    
    ## read the input file and clean up given the parameters
    clean_db = []
    discard_db = []
    count = 0
    count_clean = 0
    with open(INPUT, 'r') as file_in:
        for line in file_in:
            count = count + 1
            lines = line.rstrip('\n')
            upline = lines.upper()
            seq = upline.rsplit('\t', 1)[1]
            species = upline.split('\t')[8]
            if len(seq) >= MINLEN and len(seq) <= MAXLEN and seq.count('N') <= MAXNS and species.count('ENVIRONMENTAL') <= env and species.count('_SP.') <= spec and upline.count(',NAN') <= nans:
                count_clean = count_clean + 1
                clean_db.append(line)
            else:
                discard_db.append(line)
    
    ## write cleaned database to file
    cleaned = count - count_clean
    print(f'found {count} number of sequences in {INPUT}')
    print(f'removed {cleaned} sequences during filtering')
    print(f'written {count_clean} sequences to {OUTPUT}\n')
    with open(OUTPUT, 'w') as file_out:
        for item in clean_db:
            file_out.write(item)
    if DISCARD != 'no':
        with open(DISCARD, 'w') as dis_out:
            for item in discard_db:
                dis_out.write(item)

################################################
############# MODULE VISUALISATION #############
################################################

## figure output
def visualization(args):
    INPUT = args.input
    OUTPUT = args.output
    METHOD = args.method
    LEVEL = args.level
    SPECIES = args.species

    ## horizontal barchart
    if METHOD == 'diversity':
        tax_group_list, uniq_tax_group_list, species_dict = split_db_by_taxgroup(INPUT, LEVEL)
        sequence_counter = Counter(tax_group_list)
        list_info_dict = num_spec_seq_taxgroup(uniq_tax_group_list, species_dict, sequence_counter)
        sorted_info = sorted(list_info_dict, key = lambda i: (i['sequence']))
        figure = horizontal_barchart(sorted_info)
    
    ## length distribution
    elif METHOD == 'amplicon_length':
        amp_length_dict = get_amp_length(INPUT, LEVEL)
        figure = amplength_figure(amp_length_dict)
    
    ## phylogenetic tree
    elif METHOD == 'phylo':
        print('test')
    

    ## incorrect parameter
    else:
        print('\nplease specify method of visualization: "diversity", "amplicon_length", "phylo"\n')


## format the taxonomic lineage
def tax_format(args):
    INPUT = args.input
    OUTPUT = args.output
    FORMAT = args.format

    ## format database to sintax
    if FORMAT == 'sintax':
        print(f'\nformatting {INPUT} to sintax format\n')
        with open(OUTPUT, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    sintax = '>' + line.split('\t')[0] + ';tax=d:' + line.split('\t')[2].split(',')[2] + ',p:' + line.split('\t')[3].split(',')[2] + ',c:' + line.split('\t')[4].split(',')[2] + ',o:' + line.split('\t')[5].split(',')[2] + ',f:' + line.split('\t')[6].split(',')[2] + ',g:' + line.split('\t')[7].split(',')[2] + ',s:' + line.split('\t')[8].split(',')[2] + '\n' + line.split('\t')[9]
                    f_out.write(sintax)

    ## format database to RDP
    elif FORMAT == 'rdp':
        print(f'\nformatting {INPUT} to RDP format\n')
        with open(OUTPUT, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    rdp = '>' + line.split('\t')[0] + '\t' + 'root;' + line.split('\t')[2].split(',')[2] + ';' + line.split('\t')[3].split(',')[2] + ';' + line.split('\t')[4].split(',')[2] + ';' + line.split('\t')[5].split(',')[2] + ';' + line.split('\t')[6].split(',')[2] + ';' + line.split('\t')[7].split(',')[2] + ';' + line.split('\t')[8].split(',')[2] + '\n' + line.split('\t')[9]
                    f_out.write(rdp)

    ## format database to QIIf
    elif FORMAT == 'qiif':
        print(f'\nformatting {INPUT} to QIIf format\n')
        fasta_f = OUTPUT + '.fasta'
        txt_f = OUTPUT + '.txt'
        with open(fasta_f, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    fasta = '>' + line.split('\t')[0] + '\n' + line.split('\t')[9]
                    f_out.write(fasta)
        with open(txt_f, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:   
                    tax = line.split('\t')[0] + '\t' + 'k__' + line.split('\t')[2].split(',')[2] + ';p__' + line.split('\t')[3].split(',')[2] + ';c__' + line.split('\t')[4].split(',')[2] + ';o__' + line.split('\t')[5].split(',')[2] + ';f__' + line.split('\t')[6].split(',')[2] + ';g__' + line.split('\t')[7].split(',')[2] + ';s__' + line.split('\t')[8].split(',')[2] + '\n'
                    f_out.write(tax)

    ## format database to QIIz
    elif FORMAT == 'qiiz':
        print(f'\nformatting {INPUT} to QIIz format')
        print('still to add, not sure how this looks like')
    
    ## format database to DAD
    elif FORMAT == 'dad':
        print(f'\nformatting {INPUT} to DAD format\n')
        with open(OUTPUT, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    dad = '>' + line.split('\t')[2].split(',')[2] + ';' + line.split('\t')[3].split(',')[2] + ';' + line.split('\t')[4].split(',')[2] + ';' + line.split('\t')[5].split(',')[2] + ';' + line.split('\t')[6].split(',')[2] + ';' + line.split('\t')[7].split(',')[2] + '\n' + line.split('\t')[9]
                    f_out.write(dad)

    ## format database to DADs
    elif FORMAT == 'dads':
        print(f'\nformatting {INPUT} to DADs format\n')
        with open(OUTPUT, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    dads = '>' + line.split('\t')[0] + ' ' + line.split('\t')[7].split(',')[2] + ' ' + line.split('\t')[8].split(',')[2] + '\n' + line.split('\t')[9]
                    f_out.write(dads)
    
    ## format database to IDT
    elif FORMAT == 'idt':
        print(f'\nformatting {INPUT} to IDT format\n')
        with open(OUTPUT, 'w') as f_out:
            with open(INPUT, 'r') as f_in:
                for line in f_in:
                    idt = '>' + line.split('\t')[2].split(',')[2] + ';' + line.split('\t')[3].split(',')[2] + ';' + line.split('\t')[4].split(',')[2] + ';' + line.split('\t')[5].split(',')[2] + ';' + line.split('\t')[6].split(',')[2] + ';' + line.split('\t')[7].split(',')[2] + ';' + line.split('\t')[8].split(',')[2] + '\n' + line.split('\t')[9]
                    f_out.write(idt)

    ## unknown format specified
    else:
        print('\nplease specify one of the accepted formats: "sintax", "rdp", "qiif", "qiiz", "dad", "dads", "idt"\n')

################################################
################### ARGPARSE ###################
################################################
def main():
    parser = argparse.ArgumentParser(description = 'creating a curated reference database')
    subparser = parser.add_subparsers()

    db_download_parser = subparser.add_parser('db_download', description = 'downloading sequence data from online databases')
    db_download_parser.set_defaults(func = db_download)
    db_download_parser.add_argument('-s', '--source', help = 'specify online database used to download sequences. Currently supported options are: (1) ncbi, (2) embl, (3) mitofish, (4) bold, (5) taxonomy', dest = 'source', type = str, required = True)
    db_download_parser.add_argument('-db', '--database', help = 'specific database used to download sequences. Example NCBI: nucleotide. Example EMBL: mam*. Example BOLD: Actinopterygii', dest = 'database', type = str)
    db_download_parser.add_argument('-q', '--query', help = 'NCBI query search to limit portion of database to be downloaded. Example: "16S[All Fields] AND ("1"[SLEN] : "50000"[SLEN])"', dest = 'query', type = str)
    db_download_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str)
    db_download_parser.add_argument('-k', '--keep_original', help = 'keep original downloaded file, default = "no"', dest = 'orig', type = str, default = 'no')
    db_download_parser.add_argument('-e', '--email', help = 'email address to connect to NCBI servers', dest = 'email', type = str)
    db_download_parser.add_argument('-b', '--batchsize', help = 'number of sequences downloaded from NCBI per iteration. Default = 5000', dest = 'batchsize', type = int, default = 5000)

    db_import_parser = subparser.add_parser('db_import', description = 'import existing or curated database')
    db_import_parser.set_defaults(func = db_import)
    db_import_parser.add_argument('-i', '--input', help = 'input database filename', dest = 'input', type = str, required = True)
    db_import_parser.add_argument('-s', '--seq_header', help = 'information provided in sequence header: "accession" or "species"', dest = 'header', type = str, required = True)
    db_import_parser.add_argument('-o', '--output', help = 'output file name option', dest = 'output', type = str, required = True)
    db_import_parser.add_argument('-f', '--fwd', help = 'forward primer sequence in 5-3 direction', dest = 'fwd', type = str)
    db_import_parser.add_argument('-r', '--rev', help = 'reverse primer sequence in 5-3 direction', dest = 'rev', type = str)
    db_import_parser.add_argument('-d', '--delim', help = 'delimiter specifying species or accession', dest = 'delim', type = str, required = True)

    db_merge_parser = subparser.add_parser('db_merge', description = 'merge multiple databases')
    db_merge_parser.set_defaults(func = db_merge)
    db_merge_parser.add_argument('-i', '--input', nargs = '+', help = 'list of files to be merged', dest = 'input', required = True)
    db_merge_parser.add_argument('-u', '--uniq', help = 'keep only unique accession numbers', dest = 'uniq', type = str, default = '')
    db_merge_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str, required = True)
    
    in_silico_pcr_parser = subparser.add_parser('insilico_pcr', description = 'curating the downloaded reference sequences with an in silico PCR')
    in_silico_pcr_parser.set_defaults(func = insilico_pcr)
    in_silico_pcr_parser.add_argument('-f', '--fwd', help = 'forward primer sequence in 5-3 direction', dest = 'fwd', type = str, required = True)
    in_silico_pcr_parser.add_argument('-r', '--rev', help = 'reverse primer sequence in 5-3 direction', dest = 'rev', type = str, required = True)
    in_silico_pcr_parser.add_argument('-i', '--input', help = 'input filename', dest = 'input', type = str, required = True)
    in_silico_pcr_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str, required = True)
    in_silico_pcr_parser.add_argument('-e', '--error', help = 'number of errors allowed in primer-binding site. Default = 4.5', dest = 'error', type = str, default = '4.5')

    ref_database_parser = subparser.add_parser('assign_tax', description = 'creating the reference database with taxonomic information')
    ref_database_parser.set_defaults(func = assign_tax)
    ref_database_parser.add_argument('-i', '--input', help = 'input file containing the curated fasta sequences after in silico PCR', dest = 'input', type = str, required = True)
    ref_database_parser.add_argument('-o', '--output', help = 'curated reference database output file', dest = 'output', type = str, required = True)
    ref_database_parser.add_argument('-a', '--acc2tax', help = 'accession to taxid file name', dest = 'acc2tax', type = str, required = True)
    ref_database_parser.add_argument('-t', '--taxid', help = 'taxid file name', dest = 'taxid', type = str, required = True)
    ref_database_parser.add_argument('-n', '--name', help = 'phylogeny file name', dest = 'name', type = str, required = True)

    dereplication_parser = subparser.add_parser('dereplicate', description = 'dereplicating the database')
    dereplication_parser.set_defaults(func = dereplicate)
    dereplication_parser.add_argument('-i', '--input', help = 'filename of the curated reference database', dest = 'input', type = str, required = True)
    dereplication_parser.add_argument('-o', '--output', help = 'filename of the dereplicated curated reference database', dest = 'output', type = str, required = True)
    dereplication_parser.add_argument('-m', '--method', help = 'method of dereplication: "strict", "single_species", "uniq_species"', dest = 'method', type = str, required = True)

    seq_cleanup_parser = subparser.add_parser('seq_cleanup', description = 'filtering the database on sequence and header parameters')
    seq_cleanup_parser.set_defaults(func = db_filter)
    seq_cleanup_parser.add_argument('-min', '--minlen', help = 'minimum sequence length to be retained in the database. Default = 100', dest = 'minlen', type = int, default = '100')
    seq_cleanup_parser.add_argument('-max', '--maxlen', help = 'maximum sequence length to be retained in the database. Default = 500', dest = 'maxlen', type = int, default = '500')
    seq_cleanup_parser.add_argument('-n', '--maxns', help = 'maximum number of ambiguous bases allowed in the sequence. Default = 0', dest = 'maxns', type = int, default = '0')
    seq_cleanup_parser.add_argument('-i', '--input', help = 'input file name', dest = 'input', type = str, required = True)
    seq_cleanup_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str, required = True)
    seq_cleanup_parser.add_argument('-d', '--discard', help = 'file name of discarded sequences', dest = 'discard', type = str, default = 'no')
    seq_cleanup_parser.add_argument('-e', '--enviro', help = 'discard environmental sequences from the dataset. yes/no', dest = 'env', type = str, default = 'no')
    seq_cleanup_parser.add_argument('-s', '--species', help = 'discard sequences for which the species name is unspecified. yes/no', dest = 'spec', type = str, default = 'no')
    seq_cleanup_parser.add_argument('-na', '--nans', help = 'discard sequences with N number of unspecified taxonomic levels', dest = 'nans', type = str, default = 'no')
    
    format_database_parser = subparser.add_parser('tax_format', description = 'formatting the database to various formats')
    format_database_parser.set_defaults(func = tax_format)
    format_database_parser.add_argument('-i', '--input', help = 'input file name', dest = 'input', type = str, required = True)
    format_database_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str, required = True)
    format_database_parser.add_argument('-f', '--format', help = 'process database to format: "sintax", "rdp", "qiif", "qiiz", "dad", "dads", "idt"', dest = 'format', type = str, required = True)

    visualization_parser = subparser.add_parser('visualization', description = 'figure displaying various aspects of the reference database')
    visualization_parser.set_defaults(func = visualization)
    visualization_parser.add_argument('-i', '--input', help = 'input file name', dest = 'input', type = str, required = True)
    visualization_parser.add_argument('-o', '--output', help = 'output file name', dest = 'output', type = str, required = True)
    visualization_parser.add_argument('-m', '--method', help = 'method of visualization: "diversity", "amplicon_lenght", "phylo"', dest = 'method', type = str, required = True)
    visualization_parser.add_argument('-l', '--level', help = 'taxonomic level to split the database: "superkingdom", "phylum", "class", "order", "family", "genus", "species"', dest = 'level', type = str, required = True)
    visualization_parser.add_argument('-s', '--species', help = 'list of species of interest for phylo method', dest = 'species', type = str)

#     phylo_parser = subparser.add_parser('phylo_build', description = 'generating phylogenetic trees for species of interest')
#     phylo_parser.set_defaults(func = phylo)
#     phylo_parser.add_argument('-s', '--species', help = 'text file containing list of species separated by newlines', dest = 'species', type = str, required = True)
#     phylo_parser.add_argument('-db', '--database', help = 'curated reference database', dest = 'database', type = str, required = True)
#     phylo_parser.add_argument('-e', '--email', help = 'email address to connect to NCBI servers', dest = 'email', type = str, required = True)
#     phylo_parser.add_argument('-o', '--output', help = 'filename for output table', dest = 'output', type = str, required = True)

    args = parser.parse_args()
    args.func(args)

if __name__ == '__main__':
    main()
